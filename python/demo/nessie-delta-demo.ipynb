{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nessie Demo\n",
    "===========\n",
    "This demo showcases how to use Nessie python API along with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Pyspark + Nessie environment\n",
    "----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "from pyspark.sql import *\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from py4j.java_gateway import java_import\n",
    "findspark.init()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .config(\"spark.jars\", \"../../clients/deltalake-spark3/target/nessie-deltalake-spark3-1.0-SNAPSHOT.jar\") \\\n",
    "                    .config(\"spark.sql.execution.arrow.enabled\", \"true\") \\\n",
    "                    .config(\"spark.hadoop.fs.defaultFS\", 'file:///home/ryan/workspace/tick_data/spark_warehouse') \\\n",
    "                    .config(\"spark.hadoop.nessie.url\", \"http://localhost:19120/api/v1\") \\\n",
    "                    .config(\"spark.hadoop.nessie.ref\", \"main\") \\\n",
    "                    .config(\"spark.sql.catalog.nessie\", \"com.dremio.nessie.iceberg.spark.NessieDeltaSparkCatalog\") \\\n",
    "                    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "                    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "                    .config(\"spark.delta.logFileHandler.class\", \"com.dremio.nessie.deltalake.NessieLogFileMetaParser\") \\\n",
    "                    .config(\"spark.delta.logStore.class\", \"com.dremio.nessie.deltalake.NessieLogStore\") \\\n",
    "                    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "jvm = sc._gateway.jvm\n",
    "\n",
    "java_import(jvm, \"org.apache.spark.sql.delta.DeltaLog\")\n",
    "java_import(jvm, \"io.delta.tables.DeltaTable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up nessie branches\n",
    "----------------------------\n",
    "\n",
    "- Branch `main` already exists\n",
    "- Create branch `dev`\n",
    "- List all branches (pipe JSON result into jq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nessie_client create-branch dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nessie_client list-references | jq ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create tables under dev branch\n",
    "-------------------------------------\n",
    "\n",
    "Creating two tables under the `dev` branch:\n",
    "- region\n",
    "- nation\n",
    "\n",
    "It is not yet possible to create table using pyspark and iceberg, so Java code is used instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop_conf = sc._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"nessie.ref\", \"dev\")\n",
    "\n",
    "region_df = spark.read.load(\"data/region.parquet\")\n",
    "region_df.write.format(\"delta\").save(\"spark_warehouse/testing/region\")\n",
    "\n",
    "nation_df = spark.read.load(\"data/nation.parquet\")\n",
    "nation_df.write.format(\"delta\").save(\"spark_warehouse/testing/nation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check generated tables\n",
    "----------------------------\n",
    "   \n",
    "Check tables generated under the dev branch (and that the main branch does not have any tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nessie_client list-tables main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nessie_client list-tables dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nessie_client list-references | jq ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dev promotion\n",
    "-------------\n",
    "\n",
    "Promote dev branch promotion to main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nessie_client assign-branch main dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nessie_client list-tables main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nessie_client list-references | jq ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `etl` branch\n",
    "----------------------\n",
    "\n",
    "- Create a branch `etl` out of `main`\n",
    "- add data to nation\n",
    "- alter region\n",
    "- create table city\n",
    "- query the tables in `etl`\n",
    "- query the tables in `main`\n",
    "- promote `etl` branch to `main`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nessie_client create-branch etl -r `nessie_client show-reference main | jq .hash | sed 's/\"//g'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop_conf.set(\"nessie.ref\", \"etl\")\n",
    "Nation = Row(\"N_NATIONKEY\", \"N_NAME\", \"N_REGIONKEY\", \"N_COMMENT\")\n",
    "new_nations = spark.createDataFrame([\n",
    "    Nation(25, \"SYLDAVIA\", 3, \"King Ottokar's Sceptre\"),\n",
    "    Nation(26, \"SAN THEODOROS\", 1, \"The Picaros\")])\n",
    "new_nations.write.option('hadoop.nessie.ref', 'etl').format(\"delta\").mode(\"append\").save(\"testing.nation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the default branch\n",
    "hadoop_conf.set('nessie.ref', 'etl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating city table\n",
    "sc.getConf().set(\"spark.hadoop.nessie.ref\", \"etl\")\n",
    "spark.sql(\"CREATE TABLE city (C_CITYKEY BIGINT, C_NAME STRING, N_NATIONKEY BIGINT, C_COMMNT STRING) USING delta PARTITIONED BY (N_NATIONKEY) LOCATION 'spark_warehouse/testing/city'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nessie_client list-tables main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nessie_client list-tables etl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nessie_client list-references | jq ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nessie_client assign-branch main etl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nessie_client list-references | jq ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `experiment` branch\n",
    "--------------------------------\n",
    "\n",
    "- create `experiment` branch from `main`\n",
    "- drop `nation` table\n",
    "- add data to `region` table\n",
    "- compare `experiment` and `main` tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nessie_client create-branch experiment -r `nessie_client show-reference main | jq .hash | sed 's/\"//g'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the default branch\n",
    "hadoop_conf.set('nessie.ref', 'experiment')\n",
    "\n",
    "\n",
    "jvm.DeltaLog.clearCache()\n",
    "deltaTable = jvm.DeltaTable.forPath(\"spark_warehouse/testing/nation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"set spark.hadoop.nessie.ref=experiment\")\n",
    "spark.sql('INSERT INTO TABLE nessie.testing.region VALUES (5, \"AUSTRALIA\", \"Let\\'s hop there\", \"AUS\")')\n",
    "spark.sql('INSERT INTO TABLE nessie.testing.region VALUES (6, \"ANTARTICA\", \"It\\'s cold\", \"ANT\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nessie_client list-tables experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from nessie.testing.region\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from nessie.testing.`region@main`\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
