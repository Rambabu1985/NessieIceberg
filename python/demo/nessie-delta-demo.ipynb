{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nessie Demo\n",
    "===========\n",
    "This demo showcases how to use Nessie python API along with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Pyspark + Nessie environment\n",
    "----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "from pyspark.sql import *\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from py4j.java_gateway import java_import\n",
    "findspark.init()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .config(\"spark.jars\", \"../../clients/deltalake/spark3/target/nessie-deltalake-spark3-0.1-SNAPSHOT.jar\") \\\n",
    "                    .config(\"spark.sql.execution.pyarrow.enabled\", \"true\") \\\n",
    "                    .config(\"spark.hadoop.fs.defaultFS\", 'file://' + os.getcwd() + '/spark_warehouse') \\\n",
    "                    .config(\"spark.hadoop.nessie.url\", \"http://localhost:19120/api/v1\") \\\n",
    "                    .config(\"spark.hadoop.nessie.ref\", \"main\") \\\n",
    "                    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "                    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "                    .config(\"spark.delta.logFileHandler.class\", \"com.dremio.nessie.deltalake.NessieLogFileMetaParser\") \\\n",
    "                    .config(\"spark.delta.logStore.class\", \"com.dremio.nessie.deltalake.NessieLogStore\") \\\n",
    "                    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "jvm = sc._gateway.jvm\n",
    "\n",
    "java_import(jvm, \"org.apache.spark.sql.delta.DeltaLog\")\n",
    "java_import(jvm, \"io.delta.tables.DeltaTable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up nessie branches\n",
    "----------------------------\n",
    "\n",
    "- Branch `main` already exists\n",
    "- Create branch `dev`\n",
    "- List all branches (pipe JSON result into jq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ryan/workspace/nessie/python/demo/venv/bin/nessie\", line 10, in <module>\n",
      "    sys.exit(cli())\n",
      "  File \"/home/ryan/workspace/nessie/python/demo/venv/lib/python3.7/site-packages/click/core.py\", line 829, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "  File \"/home/ryan/workspace/nessie/python/demo/venv/lib/python3.7/site-packages/click/core.py\", line 782, in main\n",
      "    rv = self.invoke(ctx)\n",
      "  File \"/home/ryan/workspace/nessie/python/demo/venv/lib/python3.7/site-packages/click/core.py\", line 1259, in invoke\n",
      "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "  File \"/home/ryan/workspace/nessie/python/demo/venv/lib/python3.7/site-packages/click/core.py\", line 1066, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "  File \"/home/ryan/workspace/nessie/python/demo/venv/lib/python3.7/site-packages/click/core.py\", line 610, in invoke\n",
      "    return callback(*args, **kwargs)\n",
      "  File \"/home/ryan/workspace/nessie/python/demo/venv/lib/python3.7/site-packages/click/decorators.py\", line 33, in new_func\n",
      "    return f(get_current_context().obj, *args, **kwargs)\n",
      "  File \"/home/ryan/workspace/nessie/python/demo/venv/lib/python3.7/site-packages/pynessie/cli.py\", line 79, in create_branch\n",
      "    args[\"nessie\"].create_branch(branch, ref)\n",
      "  File \"/home/ryan/workspace/nessie/python/demo/venv/lib/python3.7/site-packages/pynessie/nessie_client.py\", line 54, in create_branch\n",
      "    create_branch(self._base_url, branch, ref, self._ssl_verify)\n",
      "  File \"/home/ryan/workspace/nessie/python/demo/venv/lib/python3.7/site-packages/pynessie/_endpoints.py\", line 145, in create_branch\n",
      "    _post(base_url + url, None, ssl_verify=ssl_verify)\n",
      "  File \"/home/ryan/workspace/nessie/python/demo/venv/lib/python3.7/site-packages/pynessie/_endpoints.py\", line 36, in _post\n",
      "    return _check_error(r, details)\n",
      "  File \"/home/ryan/workspace/nessie/python/demo/venv/lib/python3.7/site-packages/pynessie/_endpoints.py\", line 74, in _check_error\n",
      "    raise NessieConflictException(\"Entity already exists at \" + details, error, r)\n",
      "pynessie.error.NessieConflictException: Entity already exists at : 409 Client Error: Conflict for url: http://localhost:19120/api/v1/trees/branch/dev\n"
     ]
    }
   ],
   "source": [
    "!nessie create-branch dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;39m[\n",
      "  \u001b[1;39m{\n",
      "    \u001b[0m\u001b[34;1m\"type\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"BRANCH\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0m\u001b[34;1m\"name\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"main\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0m\u001b[34;1m\"hash\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"e0b41c30f0710277532f51242994e10acfdc46bf\"\u001b[0m\u001b[1;39m\n",
      "  \u001b[1;39m}\u001b[0m\u001b[1;39m,\n",
      "  \u001b[1;39m{\n",
      "    \u001b[0m\u001b[34;1m\"type\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"BRANCH\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0m\u001b[34;1m\"name\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"dev\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0m\u001b[34;1m\"hash\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"e0b41c30f0710277532f51242994e10acfdc46bf\"\u001b[0m\u001b[1;39m\n",
      "  \u001b[1;39m}\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!nessie list-references | jq ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create tables under dev branch\n",
    "-------------------------------------\n",
    "\n",
    "Creating two tables under the `dev` branch:\n",
    "- region\n",
    "- nation\n",
    "\n",
    "It is not yet possible to create table using pyspark and iceberg, so Java code is used instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop_conf = sc._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"nessie.ref\", \"dev\")\n",
    "\n",
    "region_df = spark.read.load(\"data/region.parquet\")\n",
    "region_df.write.format(\"delta\").save(\"spark_warehouse/testing/region\")\n",
    "\n",
    "nation_df = spark.read.load(\"data/nation.parquet\")\n",
    "nation_df.write.format(\"delta\").save(\"spark_warehouse/testing/nation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check generated tables\n",
    "----------------------------\n",
    "   \n",
    "Check tables generated under the dev branch (and that the main branch does not have any tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries(entries=[], has_more=False, token=None)\n"
     ]
    }
   ],
   "source": [
    "!nessie list-tables main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries(entries=[Entry(kind='UNKNOWN', name=EntryName(elements=['home', 'ryan', 'workspace', 'nessie', 'python', 'demo', 'spark_warehouse', 'testing', 'nation', '_delta_log'])), Entry(kind='UNKNOWN', name=EntryName(elements=['home', 'ryan', 'workspace', 'nessie', 'python', 'demo', 'spark_warehouse', 'testing', 'region', '_delta_log']))], has_more=False, token=None)\n"
     ]
    }
   ],
   "source": [
    "!nessie list-tables dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;39m[\n",
      "  \u001b[1;39m{\n",
      "    \u001b[0m\u001b[34;1m\"type\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"BRANCH\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0m\u001b[34;1m\"name\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"main\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0m\u001b[34;1m\"hash\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"e0b41c30f0710277532f51242994e10acfdc46bf\"\u001b[0m\u001b[1;39m\n",
      "  \u001b[1;39m}\u001b[0m\u001b[1;39m,\n",
      "  \u001b[1;39m{\n",
      "    \u001b[0m\u001b[34;1m\"type\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"BRANCH\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0m\u001b[34;1m\"name\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"dev\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0m\u001b[34;1m\"hash\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"0374795983216838f3febfaaf8af000b4390a784\"\u001b[0m\u001b[1;39m\n",
      "  \u001b[1;39m}\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!nessie list-references | jq ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dev promotion\n",
    "-------------\n",
    "\n",
    "Promote dev branch promotion to main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "!nessie assign-branch main dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries(entries=[Entry(kind='UNKNOWN', name=EntryName(elements=['home', 'ryan', 'workspace', 'nessie', 'python', 'demo', 'spark_warehouse', 'testing', 'nation', '_delta_log'])), Entry(kind='UNKNOWN', name=EntryName(elements=['home', 'ryan', 'workspace', 'nessie', 'python', 'demo', 'spark_warehouse', 'testing', 'region', '_delta_log']))], has_more=False, token=None)\n"
     ]
    }
   ],
   "source": [
    "!nessie list-tables main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;39m[\n",
      "  \u001b[1;39m{\n",
      "    \u001b[0m\u001b[34;1m\"name\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"main\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0m\u001b[34;1m\"type\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"BRANCH\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0m\u001b[34;1m\"hash\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"0374795983216838f3febfaaf8af000b4390a784\"\u001b[0m\u001b[1;39m\n",
      "  \u001b[1;39m}\u001b[0m\u001b[1;39m,\n",
      "  \u001b[1;39m{\n",
      "    \u001b[0m\u001b[34;1m\"name\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"dev\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0m\u001b[34;1m\"type\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"BRANCH\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0m\u001b[34;1m\"hash\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"0374795983216838f3febfaaf8af000b4390a784\"\u001b[0m\u001b[1;39m\n",
      "  \u001b[1;39m}\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!nessie list-references | jq ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `etl` branch\n",
    "----------------------\n",
    "\n",
    "- Create a branch `etl` out of `main`\n",
    "- add data to nation\n",
    "- alter region\n",
    "- create table city\n",
    "- query the tables in `etl`\n",
    "- query the tables in `main`\n",
    "- promote `etl` branch to `main`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "!nessie create-branch etl -r `nessie show-reference main | jq .hash | sed 's/\"//g'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop_conf.set(\"nessie.ref\", \"etl\")\n",
    "Nation = Row(\"N_NATIONKEY\", \"N_NAME\", \"N_REGIONKEY\", \"N_COMMENT\")\n",
    "new_nations = spark.createDataFrame([\n",
    "    Nation(25, \"SYLDAVIA\", 3, \"King Ottokar's Sceptre\"),\n",
    "    Nation(26, \"SAN THEODOROS\", 1, \"The Picaros\")])\n",
    "new_nations.write.option('hadoop.nessie.ref', 'etl').format(\"delta\").mode(\"append\").save(\"testing.nation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# changing the default branch\n",
    "hadoop_conf.set('nessie.ref', 'etl')\n",
    "base_table = os.getcwd() + \"/spark_warehouse/testing/\"\n",
    "spark.sql(\"ALTER TABLE delta.`\" + base_table + \"region` ADD COLUMNS (R_ABBREV STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating city table\n",
    "sc.getConf().set(\"spark.hadoop.nessie.ref\", \"etl\")\n",
    "spark.sql(\"CREATE TABLE city (C_CITYKEY BIGINT, C_NAME STRING, N_NATIONKEY BIGINT, C_COMMNT STRING) USING delta PARTITIONED BY (N_NATIONKEY) LOCATION 'spark_warehouse/testing/city'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[EntryName(elements=['home', 'ryan', 'workspace', 'nessie', 'python', 'demo', 'spark_warehouse', 'testing', 'nation', '_delta_log']),\n",
       " EntryName(elements=['home', 'ryan', 'workspace', 'nessie', 'python', 'demo', 'spark_warehouse', 'testing', 'region', '_delta_log'])]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pynessie import init\n",
    "nessie = init()\n",
    "[i.name for i in nessie.list_tables('main').entries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[EntryName(elements=['home', 'ryan', 'workspace', 'nessie', 'python', 'demo', 'spark_warehouse', 'testing', 'city', '_delta_log']),\n",
       " EntryName(elements=['home', 'ryan', 'workspace', 'nessie', 'python', 'demo', 'spark_warehouse', 'testing', 'nation', '_delta_log']),\n",
       " EntryName(elements=['home', 'ryan', 'workspace', 'nessie', 'python', 'demo', 'spark_warehouse', 'testing', 'region', '_delta_log']),\n",
       " EntryName(elements=['home', 'ryan', 'workspace', 'nessie', 'python', 'demo', 'testing.nation', '_delta_log'])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i.name for i in nessie.list_tables('etl').entries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'main': '0374795983216838f3febfaaf8af000b4390a784',\n",
       " 'dev': '0374795983216838f3febfaaf8af000b4390a784',\n",
       " 'etl': '5965f6e72fe80fdb7f99d9d07f26eff0298f6888'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{i.name:i.hash_ for i in nessie.list_references()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nessie.assign('main','etl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'main': '5965f6e72fe80fdb7f99d9d07f26eff0298f6888',\n",
       " 'dev': '0374795983216838f3febfaaf8af000b4390a784',\n",
       " 'etl': '5965f6e72fe80fdb7f99d9d07f26eff0298f6888'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{i.name:i.hash_ for i in nessie.list_references()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `experiment` branch\n",
    "--------------------------------\n",
    "\n",
    "- create `experiment` branch from `main`\n",
    "- drop `nation` table\n",
    "- add data to `region` table\n",
    "- compare `experiment` and `main` tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "!nessie create-branch experiment -r `nessie show-reference main | jq .hash | sed 's/\"//g'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the default branch\n",
    "hadoop_conf.set('nessie.ref', 'experiment')\n",
    "\n",
    "\n",
    "jvm.DeltaLog.clearCache()\n",
    "deltaTable = jvm.DeltaTable.forPath(\"spark_warehouse/testing/nation\")\n",
    "deltaTable.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"set spark.hadoop.nessie.ref=experiment\")\n",
    "spark.sql('INSERT INTO TABLE delta.`' + base_table + 'region` VALUES (5, \"AUSTRALIA\", \"Let\\'s hop there\", \"AUS\")')\n",
    "spark.sql('INSERT INTO TABLE delta.`' + base_table + 'region` VALUES (6, \"ANTARTICA\", \"It\\'s cold\", \"ANT\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries(entries=[Entry(kind='UNKNOWN', name=EntryName(elements=['home', 'ryan', 'workspace', 'nessie', 'python', 'demo', 'spark_warehouse', 'testing', 'city', '_delta_log'])), Entry(kind='UNKNOWN', name=EntryName(elements=['home', 'ryan', 'workspace', 'nessie', 'python', 'demo', 'spark_warehouse', 'testing', 'nation', '_delta_log'])), Entry(kind='UNKNOWN', name=EntryName(elements=['home', 'ryan', 'workspace', 'nessie', 'python', 'demo', 'spark_warehouse', 'testing', 'region', '_delta_log'])), Entry(kind='UNKNOWN', name=EntryName(elements=['home', 'ryan', 'workspace', 'nessie', 'python', 'demo', 'testing.nation', '_delta_log']))], has_more=False, token=None)\n"
     ]
    }
   ],
   "source": [
    "!nessie list-tables experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>r_regionkey</th>\n",
       "      <th>r_name</th>\n",
       "      <th>r_comment</th>\n",
       "      <th>R_ABBREV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>AFRICA</td>\n",
       "      <td>lar deposits. blithely final packages cajole. ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>AMERICA</td>\n",
       "      <td>hs use ironic, even requests. s</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ASIA</td>\n",
       "      <td>ges. thinly even pinto beans ca</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>EUROPE</td>\n",
       "      <td>ly final courts cajole furiously final excuse</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>MIDDLE EAST</td>\n",
       "      <td>uickly special accounts cajole carefully blith...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>Let's hop there</td>\n",
       "      <td>AUS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>ANTARTICA</td>\n",
       "      <td>It's cold</td>\n",
       "      <td>ANT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   r_regionkey       r_name  \\\n",
       "0            0       AFRICA   \n",
       "1            1      AMERICA   \n",
       "2            2         ASIA   \n",
       "3            3       EUROPE   \n",
       "4            4  MIDDLE EAST   \n",
       "5            5    AUSTRALIA   \n",
       "6            6    ANTARTICA   \n",
       "\n",
       "                                           r_comment R_ABBREV  \n",
       "0  lar deposits. blithely final packages cajole. ...     None  \n",
       "1                    hs use ironic, even requests. s     None  \n",
       "2                    ges. thinly even pinto beans ca     None  \n",
       "3      ly final courts cajole furiously final excuse     None  \n",
       "4  uickly special accounts cajole carefully blith...     None  \n",
       "5                                    Let's hop there      AUS  \n",
       "6                                          It's cold      ANT  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from delta.`\" + base_table + \"region`\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The branch used for Delta queries should be changed manually to query a different branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>r_regionkey</th>\n",
       "      <th>r_name</th>\n",
       "      <th>r_comment</th>\n",
       "      <th>R_ABBREV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>AFRICA</td>\n",
       "      <td>lar deposits. blithely final packages cajole. ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>AMERICA</td>\n",
       "      <td>hs use ironic, even requests. s</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ASIA</td>\n",
       "      <td>ges. thinly even pinto beans ca</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>EUROPE</td>\n",
       "      <td>ly final courts cajole furiously final excuse</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>MIDDLE EAST</td>\n",
       "      <td>uickly special accounts cajole carefully blith...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   r_regionkey       r_name  \\\n",
       "0            0       AFRICA   \n",
       "1            1      AMERICA   \n",
       "2            2         ASIA   \n",
       "3            3       EUROPE   \n",
       "4            4  MIDDLE EAST   \n",
       "\n",
       "                                           r_comment R_ABBREV  \n",
       "0  lar deposits. blithely final packages cajole. ...     None  \n",
       "1                    hs use ironic, even requests. s     None  \n",
       "2                    ges. thinly even pinto beans ca     None  \n",
       "3      ly final courts cajole furiously final excuse     None  \n",
       "4  uickly special accounts cajole carefully blith...     None  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hadoop_conf.set('nessie.ref', 'main')\n",
    "jvm.DeltaLog.clearCache()\n",
    "spark.sql(\"set spark.hadoop.nessie.ref=main\")\n",
    "spark.sql(\"select * from delta.`/home/ryan/workspace/nessie/python/demo/spark_warehouse/testing/region`\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
